//sbt
//build.properties
name:="SparkExe"
version:="1.0"
scalaVersion:="2.11.8"
LibraryDependencies++=Seq( "org.apache.spark" %% "spark-core" % "2.0.0",
                            "org.apache.spark" %% "spark-sql" % "2.0.0)


package sparkproject

import org.apache.spark.sql.SparkSession

object Sparkexe{
        def main(args:Array[String]):Unit {
            val spark = SparkSession.builder()
                        .appname("creating spark project")
                        .master("yarn")
                        .config("spark.yarn.jars","hdfs://192.168.31.14:8020/user/cloudera/jars/*.jar")
                        .enableHiveSupport()
                        .getOrCreate()
                        
                        
 val parquetdf = spark.read.parquet("/users/cloudera/sample_data/parq.parquet")
 
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions._

val w = Window.partitionBy($"entity_id")
val df2 =(df.groupBy("entity_id","item_id","month_id").agg(sum($"signal_count").as("count1")).
         withColumn("total_signals",sum($"count1").over(w))).drop("count1")

val df3 = df2.select("entity_id","item_id","month_id","total_signals").
            withColumn("new",lead(("item_id"),1,0).over(Window.partitionBy("month_id").orderBy("item_id"))).
            withColumn("old",lag(("item_id"),1,0).over(Window.partitionBy("month_id").orderBy("item_id"))).
            withColumn("rn",row_number.over(Window.partitionBy("entity_id").orderBy('item_id))).filter($"rn"===1).
            select("entity_id","old","new","total_signals")

df3.show

df3.repartition(1).write.parquet("/user/cloudera/sample_data/result.parquet")
 
spark.stop()
  
  

 
 
  
  
  
                   
                   
       
 
 
 
 
 
 
 
 
                        

         
